name: Convert Gemma4B to CoreML

on:
  workflow_dispatch:

permissions:
  contents: write
  actions: write

jobs:
  convert:
    runs-on: macos-13

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install deps
        run: |
          pip install -U "transformers<5" coremltools optimum-coreml huggingface_hub

      - name: Download Gemma 3-4B QAT 4bit
        run: |
          python - <<'PY'
          import os
          from transformers import AutoModelForCausalLM, AutoTokenizer
          model_id = "mlx-community/gemma-3-4b-it-qat-4bit"
          save_dir = "gemma4b_qat"
          tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
          mod = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, trust_remote_code=True)
          tok.save_pretrained(save_dir); mod.save_pretrained(save_dir)
          print(">> model saved to", save_dir)
          PY

      - name: Export to CoreML
        run: |
          python - <<'PY'
          import pathlib
          from optimum.coreml import CoreMLModel
          from transformers import AutoModelForCausalLM

          model_dir = "gemma4b_qat"
          out_dir = pathlib.Path("gemma4b_coreml")
          out_dir.mkdir(exist_ok=True)

          print(">> loading HF model...")
          hf_model = AutoModelForCausalLM.from_pretrained(model_dir)
          print(">> exporting to CoreML (int4)...")
          coreml_model = CoreMLModel.from_transformers(hf_model, output_dir=out_dir, quantize=True)
          coreml_model.save_precompiled(out_dir / "gemma4b_q4.mlmodel")
          print(">> saved CoreML to", out_dir)
          PY

      - name: Zip model folder
        run: |
          zip -r gemma4b_coreml.zip gemma4b_coreml

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: gemma4b_coreml
          path: gemma4b_coreml.zip
